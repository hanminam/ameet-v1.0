# src/app/services/discussion_flow.py

import asyncio
import json
from typing import Dict, List, Literal, Optional
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai import ChatVertexAI
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from app.tools.search import perform_web_search_async, web_search_tool
from datetime import datetime
from langchain_core.messages import BaseMessage
from app.schemas.orchestration import AgentDetail

from pydantic import BaseModel, ValidationError

from app import db
from app.db import redis_client

from app.schemas.orchestration import DebateTeam
from app.models.discussion import AgentSettings, DiscussionLog
from app.services.utility_agents import run_snr_agent, run_verifier_agent
from app.core.config import logger

from app.schemas.orchestration import AgentDetail # AgentDetail ìŠ¤í‚¤ë§ˆ ì¶”ê°€
from app.schemas.discussion import VoteContent
from app.schemas.discussion import InteractionAnalysisResult
import re

# ì—ì´ì „íŠ¸ ë° ë„êµ¬ ì‹¤í–‰ì„ ìœ„í•œ LangChain ëª¨ë“ˆ ì„í¬íŠ¸
from langchain.agents import AgentExecutor, create_tool_calling_agent
from app.tools.search import available_tools

# ReAct íŒ¨í„´ì„ ì ìš©í•œ ê°•ë ¥í•œ ì‹œìŠ¤í…œ ë ˆë²¨ ë„êµ¬ ì‚¬ìš© ê·œì¹™ ì •ì˜
SYSTEM_TOOL_INSTRUCTION_BLOCK = """
---
### Tools Guide (VERY IMPORTANT)

You have access to a `web_search` tool. Your decision to use it must follow a strict 2-step process:

**Step 1: Evaluate Provided Information**
First, thoroughly review the "[ì¤‘ì•™ ì§‘ì¤‘ì‹ ì›¹ ê²€ìƒ‰ ê²°ê³¼]" and "[ì°¸ê³  ìë£Œ]" provided in the prompt. This is the baseline information for the current turn.

**Step 2: Justify and Execute Supplemental Search (If Necessary)**
You are authorized to use the `web_search` tool **ONLY IF** the provided information is insufficient for you to perform your specific role as an expert.

-   **Justification (Internal Thought):** Before calling the tool, you must internally reason why a supplemental search is critical. For example: "As a Financial Analyst, the general overview of robotaxis is not enough. I need specific, recent financial data."
-   **Execution:** If justified, perform **one, highly-specific** search to acquire the missing information. Do NOT repeat the general search that was already provided.

**CRITICAL RULES:**
-   **DO NOT** use the `web_search` tool if the provided information is sufficient.
-   Your final answer MUST be a complete, natural language response, not a JSON object.
-   Do not mention your tool usage in your final answer. Integrate the findings naturally into your argument.
---
"""

# ì…ì¥ ë³€í™” ë¶„ì„ ê²°ê³¼ Pydantic ëª¨ë¸
class StanceAnalysis(BaseModel):
    change: Literal['ìœ ì§€', 'ê°•í™”', 'ìˆ˜ì •', 'ì•½í™”']
    reason: str

# ê²€ìƒ‰ ì½”ë””ë„¤ì´í„°ë¥¼ í˜¸ì¶œí•˜ëŠ” ìƒˆë¡œìš´ ë‚´ë¶€ í•¨ìˆ˜
async def _get_search_query(discussion_log: DiscussionLog, user_vote: Optional[str]) -> Optional[str]:
    try:
        coordinator_setting = await AgentSettings.find_one(
            AgentSettings.name == "Search Coordinator", AgentSettings.status == "active"
        )
        if not coordinator_setting:
            logger.warning("!!! 'Search Coordinator' ì—ì´ì „íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¤‘ì•™ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return None

        history_str = "\n\n".join([f"{t['agent_name']}: {t['message']}" for t in discussion_log.transcript])
        
        human_prompt = (
            f"í† ë¡  ì£¼ì œ: {discussion_log.topic}\n\n"
            f"ì§€ê¸ˆê¹Œì§€ì˜ í† ë¡  ë‚´ìš©:\n{history_str}\n\n"
            f"ì‚¬ìš©ìì˜ ë‹¤ìŒ ë¼ìš´ë“œ ì§€ì‹œì‚¬í•­: '{user_vote}'\n\n"
            "ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒ í† ë¡ ì— ê°€ì¥ ë„ì›€ì´ ë  ë‹¨ í•˜ë‚˜ì˜ ì›¹ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."
        )

        llm = ChatGoogleGenerativeAI(model=coordinator_setting.config.model)
        prompt = ChatPromptTemplate.from_messages([
            ("system", coordinator_setting.config.prompt),
            ("human", "{input}")
        ])
        chain = prompt | llm
        
        response = await chain.ainvoke(
            {"input": human_prompt},
            config={"tags": [f"discussion_id:{discussion_log.discussion_id}", "task:generate_search_query"]}
        )
        query = response.content.strip()
        logger.info(f"--- [Search Coordinator] ìƒì„±ëœ ê²€ìƒ‰ì–´: {query} ---")
        return query

    except Exception as e:
        logger.error(f"!!! 'Search Coordinator' ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return None

# ê°œë³„ ì—ì´ì „íŠ¸ì˜ ì…ì¥ ë³€í™”ë¥¼ ë¶„ì„í•˜ëŠ” AI í˜¸ì¶œ í•¨ìˆ˜
async def _get_single_stance_change(
    agent_name: str, prev_statement: str, current_statement: str, discussion_id: str, turn_number: int
) -> dict:
    logger.info(f"--- [Stance Analysis] Agent: {agent_name}, Turn: {turn_number} ë¶„ì„ ì‹œì‘ ---")
    try:
        analyst_setting = await AgentSettings.find_one(
            AgentSettings.name == "Stance Analyst", AgentSettings.status == "active"
        )
        # 1. Stance Analyst ì—ì´ì „íŠ¸ë¥¼ DBì—ì„œ ì°¾ì•˜ëŠ”ì§€ í™•ì¸
        if not analyst_setting:
            # logger.warning("!!! [Stance Analysis] 'Stance Analyst' ì—ì´ì „íŠ¸ë¥¼ DBì—ì„œ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ 'active' ìƒíƒœê°€ ì•„ë‹™ë‹ˆë‹¤.")
            return {"agent_name": agent_name, "change": "ë¶„ì„ ë¶ˆê°€", "icon": "â“"}
        # logger.info("'Stance Analyst' ì—ì´ì „íŠ¸ ì„¤ì •ì„ ì„±ê³µì ìœ¼ë¡œ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

        transcript_to_analyze = (
            f"ì—ì´ì „íŠ¸ ì´ë¦„: {agent_name}\n\n"
            f"ì´ì „ ë°œì–¸: \"{prev_statement}\"\n\n"
            f"í˜„ì¬ ë°œì–¸: \"{current_statement}\""
        )
        # 2. AIì—ê²Œ ì „ë‹¬ë  ìµœì¢… í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì„ ë¡œê·¸ë¡œ ì¶œë ¥
        # logger.info(f"--- AIì—ê²Œ ì „ë‹¬ë  í”„ë¡¬í”„íŠ¸ ---\n{transcript_to_analyze}\n---------------------------")

        analyst_agent = ChatGoogleGenerativeAI(model=analyst_setting.config.model)
        structured_llm = analyst_agent.with_structured_output(StanceAnalysis)
        prompt = ChatPromptTemplate.from_messages([
            ("system", analyst_setting.config.prompt),
            ("human", "ë‹¤ìŒ í† ë¡  ëŒ€í™”ë¡ì„ ë¶„ì„í•˜ì„¸ìš”:\n\n{transcript}")
        ])
        chain = prompt | structured_llm
        
        analysis = await chain.ainvoke(
            {"transcript": transcript_to_analyze},
            config={"tags": [f"discussion_id:{discussion_id}", f"turn:{turn_number}", "task:stance_analysis"]}
        )
        
        # 3. AIì˜ ì‘ë‹µì´ ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±ë˜ì—ˆëŠ”ì§€ í™•ì¸
        # logger.info(f"ì„±ê³µì ìœ¼ë¡œ AI ì‘ë‹µì„ íŒŒì‹±í–ˆìŠµë‹ˆë‹¤: {analysis}")
        
        icon_map = {"ìœ ì§€": "ğŸ˜", "ê°•í™”": "ğŸ”¼", "ìˆ˜ì •": "ğŸ”„", "ì•½í™”": "ğŸ”½"}
        return {"agent_name": agent_name, "change": analysis.change, "icon": icon_map.get(analysis.change, "â“")}
    
    except Exception as e:
        # 4. ì˜¤ë¥˜ ë°œìƒ ì‹œ, ì •í™•í•œ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥
        logger.error(f"!!! [Stance Analysis] ì—ëŸ¬ ë°œìƒ: Agent '{agent_name}'ì˜ ì…ì¥ ë¶„ì„ ì¤‘ ì‹¤íŒ¨. ì—ëŸ¬: {e}", exc_info=True)
        return {"agent_name": agent_name, "change": "ë¶„ì„ ë¶ˆê°€", "icon": "â“"}

# ëª¨ë“  ì°¸ì—¬ìì˜ ì…ì¥ ë³€í™”ë¥¼ ë³‘ë ¬ë¡œ ë¶„ì„í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
async def _analyze_stance_changes(transcript: List[dict], jury_members: List[dict], discussion_id: str, turn_number: int) -> List[dict]:
    """
    [ê³ ë„í™” ë²„ì „] ì „ì²´ í† ë¡  ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ê° ì—ì´ì „íŠ¸ì˜ ìµœì‹  ë°œì–¸ 2ê°œë¥¼ ì •í™•íˆ ì°¾ì•„
    ì…ì¥ ë³€í™”ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    logger.info(f"--- [Stance Analysis] Robust analysis started for Turn: {turn_number} ---")

    # 1. í† ë¡ ì´ ìµœì†Œ 1ë¼ìš´ë“œëŠ” ì§„í–‰ë˜ì–´ì•¼ ë¹„êµê°€ ê°€ëŠ¥í•˜ë¯€ë¡œ ì´ ì¡°ê±´ì€ ìœ íš¨í•©ë‹ˆë‹¤.
    if turn_number < 1:
        logger.warning(f"Analysis conditions not met (Turn: {turn_number} < 1). Skipping analysis.")
        return []

    # 2. [í•µì‹¬ ë¡œì§] ì „ì²´ transcriptë¥¼ ìˆœíšŒí•˜ë©° ì „ë¬¸ê°€ ì—ì´ì „íŠ¸ë“¤ì˜ ë°œì–¸ë§Œ ë”°ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    jury_names = {member['name'] for member in jury_members}
    statements_by_agent = {name: [] for name in jury_names}

    for turn in transcript:
        agent_name = turn.get('agent_name')
        if agent_name in jury_names:
            statements_by_agent[agent_name].append(turn['message'])

    # 3. ê° ì—ì´ì „íŠ¸ë³„ë¡œ 2ê°œ ì´ìƒì˜ ë°œì–¸ì´ ìŒ“ì˜€ëŠ”ì§€ í™•ì¸í•˜ê³  ë¶„ì„ ì‘ì—…ì„ ìƒì„±í•©ë‹ˆë‹¤.
    tasks = []
    for agent_config in jury_members:
        agent_name = agent_config['name']
        agent_statements = statements_by_agent[agent_name]

        # í•´ë‹¹ ì—ì´ì „íŠ¸ì˜ ë°œì–¸ì´ 2ê°œ ì´ìƒì¼ ê²½ìš°ì—ë§Œ ë¶„ì„ ëª©ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        if len(agent_statements) >= 2:
            prev_statement = agent_statements[-2]   # ë’¤ì—ì„œ ë‘ ë²ˆì§¸ ë°œì–¸
            current_statement = agent_statements[-1] # ê°€ì¥ ìµœì‹  ë°œì–¸

            task = _get_single_stance_change(
                agent_name,
                prev_statement,
                current_statement,
                discussion_id,
                turn_number
            )
            tasks.append(task)
        else:
            logger.info(f"--- [Stance Analysis] Agent '{agent_name}' has only {len(agent_statements)} statement(s), not enough for comparison yet.")

    if not tasks:
        logger.warning("No agents have enough statements for stance change analysis in this round.")
        return []

    # 4. ìƒì„±ëœ ë¶„ì„ ì‘ì—…ë“¤ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    results = await asyncio.gather(*tasks)
    logger.info(f"--- [Stance Analysis] Analysis complete. Returning {len(results)} results. ---")

    return results

# ë¼ìš´ë“œ ìš”ì•½ ë¶„ì„ì„ ìœ„í•œ Pydantic ëª¨ë¸
class CriticalUtterance(BaseModel):
    agent_name: str
    message: str

async def _get_round_summary(transcript_str: str, discussion_id: str, turn_number: int) -> dict:
    """ë¼ìš´ë“œ ëŒ€í™”ë¡ì„ ë¶„ì„í•˜ì—¬ ê²°ì •ì  ë°œì–¸ì„ ì„ ì •í•˜ëŠ” AI ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤."""
    try:
        # DBì—ì„œ Round Analyst ì—ì´ì „íŠ¸ ì„¤ì •ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        analyst_setting = await AgentSettings.find_one(
            AgentSettings.name == "Round Analyst", AgentSettings.status == "active"
        )
        if not analyst_setting: return None

        analyst_agent = ChatGoogleGenerativeAI(model=analyst_setting.config.model)
        structured_llm = analyst_agent.with_structured_output(CriticalUtterance)
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", analyst_setting.config.prompt),
            ("human", "Analyze the following transcript:\n\n{transcript}")
        ])
        
        chain = prompt | structured_llm
        summary = await chain.ainvoke(
            {"transcript": transcript_str},
            config={"tags": [f"discussion_id:{discussion_id}", f"turn:{turn_number}", "task:summarize"]}
        )
        return summary.dict()
    except Exception as e:
        logger.error(f"Error getting round summary: {e}")
        return None

async def _run_single_agent_turn(
    agent_config: dict,
    topic: str,
    history: str,
    evidence: str,
    special_directive: str,
    discussion_id: str,
    turn_count: int
) -> str:
    """ë‹¨ì¼ ì—ì´ì „íŠ¸ì˜ ë°œì–¸(turn)ì„ ìƒì„±í•©ë‹ˆë‹¤. ëª¨ë“  ì—ì´ì „íŠ¸ëŠ” í•„ìš” ì‹œ ì›¹ ê²€ìƒ‰ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
    agent_name = agent_config.get("name", "Unknown Agent")
    logger.info(f"--- [Flow] Running turn for agent: {agent_name} (Discussion: {discussion_id}, Turn: {turn_count}) ---")

    try:
        #llm = ChatGoogleGenerativeAI(
        #    model=agent_config.get("model", "gemini-1.5-pro"),
        #    temperature=agent_config.get("temperature", 0.2)
        #)
        # í—¬í¼ í•¨ìˆ˜ë¥¼ í†µí•´ ëª¨ë¸ ì´ë¦„ì— ë§ëŠ” í´ë¼ì´ì–¸íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜´
        model_name = agent_config.get("model", "gemini-1.5-pro")
        temperature = agent_config.get("temperature", 0.2)
        llm = get_llm_client(model_name=model_name, temperature=temperature)

        human_instruction = (
            "ì§€ê¸ˆì€ 'ëª¨ë‘ ë³€ë¡ ' ì‹œê°„ì…ë‹ˆë‹¤. ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¹ì‹ ì˜ ì´ˆê¸° ì…ì¥ì„ ìµœì†Œ 100ìì—ì„œ ìµœëŒ€ 300ì ì´ë‚´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
            if turn_count == 0 else
            f"ì§€ê¸ˆì€ '{turn_count + 1}ì°¨ í† ë¡ ' ì‹œê°„ì…ë‹ˆë‹¤. ì´ì „ì˜ ì—ì´ì „íŠ¸ë“¤ì˜ ì˜ê²¬ì„ ê³ ë ¤í•˜ì—¬ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ ì£¼ì¥ì„ ë°˜ë°•í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ ì˜ê²¬ì— ì ê·¹ ë™ì¡°í•˜ê±°ë‚˜ ì•„ë‹ˆë©´ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ ì˜ê²¬ì„ ìˆ˜ë ´í•˜ì—¬ ì˜ê²¬ì„ ìˆ˜ì •í•œ ë‹¹ì‹ ì˜ ì˜ê²¬ì„ ì£¼ì¥í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ ë…¼ë¦¬ì  ëª¨ìˆœì´ë‚˜ ì‚¬ì‹¤ì— ìœ„ë°°ë˜ëŠ” ì£¼ì¥ì´ ìˆë‹¤ê³  ìƒê°í•œë‹¤ë©´ ì ê·¹ì ìœ¼ë¡œ ë°˜ë°•í•˜ì‹­ì‹œìš”. ë‹¤ë¥¸ ì—ì´ì „íŠ¸ê°€ ìƒê°í•˜ì§€ ëª»í•˜ëŠ” ìƒˆë¡œìš´ ì•„ì´ë””ì–´, ë…ì°½ì ì¸ ì£¼ì¥, ê·¸ë¦¬ê³  í† ë¡ ì˜ ì£¼ì œë¥¼ ì‹¬í™”í•  ìˆ˜ ìˆë‹¤ê³  ìƒê°ë˜ëŠ” ë‚´ìš©ì„ ì ê·¹ì ìœ¼ë¡œ ì£¼ì¥í•©ë‹ˆë‹¤. ë˜í•œ, ì´ì „ í† ë¡  ì°¨ìˆ˜ì—ì„œ ì£¼ì¥í•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìì‹ ì˜ ì£¼ì¥ì¤‘ì— ë³´ë‹¤ êµ¬ì²´ì ì¸ ëŒ€ì•ˆ, êµ¬ì²´ì ì¸ ë°©ì•ˆë“±ìœ¼ë¡œ ìì‹ ì˜ ì£¼ì¥ì„ ì‹¬í™” ë°œì „í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. í† ë¡ ì˜ ì°¨ìˆ˜ê°€ ë†’ì•„ì§ˆìˆ˜ë¡ ì´ì „ ìì‹ ì˜ ì£¼ì¥ì„ ë™ì–´ë°˜ë³µí•˜ê¸° ë³´ë‹¨ ë³´ë‹¤ êµ¬ì²´ì ì¸ ëŒ€ì•ˆì„ ì£¼ì¥í•©ë‹ˆë‹¤. ì£¼ì¥ì€ ìµœì†Œ 100ì ìµœëŒ€ 300ì ì´ë‚´ë¡œ ì¶”ê°€í•´ì£¼ì„¸ìš”."
        )

        final_human_prompt = (
            f"ë‹¹ì‹ ì€ ë‹¤ìŒ í† ë¡ ì— ì°¸ì—¬í•˜ëŠ” AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì°¸ê³  ìë£Œì™€ í† ë¡  ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¹ì‹ ì˜ ì„ë¬´ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”.\n\n"
            f"### ì „ì²´ í† ë¡  ì£¼ì œ: {topic}\n\n"
            f"### ì°¸ê³  ìë£Œ (ì´ˆê¸° ë¶„ì„ ì •ë³´)\n{evidence}\n"
            f"### ì§€ê¸ˆê¹Œì§€ì˜ í† ë¡  ë‚´ìš©:\n{history if history else 'ì•„ì§ í† ë¡  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.'}\n\n"
            f"{special_directive}\n"
            f"### ë‹¹ì‹ ì˜ ì„ë¬´\n{human_instruction}"
        )

        logger.info(f"--- [Flow] Agent '{agent_name}' will now decide on tool usage autonomously. ---")
        
        original_system_prompt = agent_config.get("prompt", "You are a helpful assistant.")
        tool_system_prompt = SYSTEM_TOOL_INSTRUCTION_BLOCK + "\n\n" + original_system_prompt

        prompt = ChatPromptTemplate.from_messages([
            ("system", tool_system_prompt),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
        
        tools = [web_search_tool]
        agent = create_tool_calling_agent(llm, tools, prompt)
        agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
        
        response = await agent_executor.ainvoke(
            {"input": final_human_prompt},
            config={"tags": [f"discussion_id:{discussion_id}", f"agent_name:{agent_name}", f"turn:{turn_count}"]}
        )
        output = response.get("output", "ì˜¤ë¥˜: ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        # 1. LangChain ì‘ë‹µì´ AIMessage ê°™ì€ ê°ì²´ì¼ ê²½ìš°, .content ì†ì„±ì„ ë¨¼ì € ì¶”ì¶œí•©ë‹ˆë‹¤.
        if isinstance(output, BaseMessage):
            content = output.content
        else:
            content = output

        # 2. contentê°€ Anthropic ëª¨ë¸ì˜ ì‘ë‹µ í˜•ì‹ì¸ listì¼ ê²½ìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        #    ex: [{'type': 'text', 'text': '...'}]
        if isinstance(content, list):
            # ë¦¬ìŠ¤íŠ¸ ì•ˆì˜ ë”•ì…”ë„ˆë¦¬ì—ì„œ 'text' í‚¤ë¥¼ ê°€ì§„ ê°’ë“¤ì„ ëª¨ë‘ ì°¾ì•„ í•©ì¹©ë‹ˆë‹¤.
            return "\n".join(
                block.get("text", "") for block in content if isinstance(block, dict) and block.get("type") == "text"
            )
        
        # 3. Gemini, OpenAI ë“±ì˜ ì¼ë°˜ì ì¸ ë¬¸ìì—´ ì‘ë‹µì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        return str(content)

    except Exception as e:
        logger.error(f"--- [Flow Error] Agent '{agent_name}' turn failed: {e} ---", exc_info=True)
        return f"({agent_name} ë°œì–¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ)"
    
# í† ë¡  íë¦„ë„ ë¶„ì„ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜
async def _analyze_flow_data(transcript: List[dict], jury_members: List[dict], discussion_id: str, turn_number: int) -> dict:
    """
    LLM ê¸°ë°˜ 'Interaction Analyst'ë¥¼ ì‚¬ìš©í•˜ì—¬ í† ë¡ ì˜ ìƒí˜¸ì‘ìš©ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    logger.info(f"--- [Flow Analysis] LLM-based interaction analysis started for Turn: {turn_number} ---")
    
    # 1. í˜„ì¬ ë¼ìš´ë“œì˜ ëŒ€í™”ë§Œ ë¬¸ìì—´ë¡œ ì¶”ì¶œ
    current_round_transcript_str = "\n\n".join(
        [f"{turn['agent_name']}: {turn['message']}" for turn in transcript[-len(jury_members):]]
    )

    try:
        # 2. DBì—ì„œ Interaction Analyst ì—ì´ì „íŠ¸ ì„¤ì •ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        analyst_setting = await AgentSettings.find_one(
            AgentSettings.name == "Interaction Analyst", AgentSettings.status == "active"
        )
        if not analyst_setting:
            logger.error("!!! [Flow Analysis] 'Interaction Analyst' ì—ì´ì „íŠ¸ë¥¼ DBì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {"interactions": []}

        # 3. LLM ë° ì²´ì¸ êµ¬ì„±
        llm = ChatGoogleGenerativeAI(model=analyst_setting.config.model, temperature=analyst_setting.config.temperature)
        structured_llm = llm.with_structured_output(InteractionAnalysisResult)
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", analyst_setting.config.prompt),
            ("human", "Please analyze the following transcript:\n\n{transcript}")
        ])
        chain = prompt | structured_llm

        # 4. LLM í˜¸ì¶œí•˜ì—¬ ë¶„ì„ ì‹¤í–‰
        analysis_result = await chain.ainvoke(
            {"transcript": current_round_transcript_str},
            config={"tags": [f"discussion_id:{discussion_id}", f"turn:{turn_number}", "task:flow_analysis"]}
        )

        # 5. Pydantic ëª¨ë¸ì„ í”„ë¡ íŠ¸ì—”ë“œê°€ ì‚¬ìš©í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        # Pydantic ëª¨ë¸ì˜ alias('from', 'to', 'type')ë¥¼ ì›ë˜ í•„ë“œëª…ìœ¼ë¡œ ë³€í™˜
        interactions_list = [
            {
                "from": interaction.from_agent,
                "to": interaction.to_agent,
                "type": interaction.interaction_type
            }
            for interaction in analysis_result.interactions
        ]

        # --- ì¤‘ë³µëœ ìƒí˜¸ì‘ìš©ì„ ì œê±°í•˜ëŠ” ë¡œì§ ì¶”ê°€ ---
        # (from, to)ë¥¼ í‚¤ë¡œ ì‚¬ìš©í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ê³„ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
        prioritized_interactions = {}

        for interaction in interactions_list:
            # (from, to) ìŒì„ ê³ ìœ  í‚¤ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
            interaction_key = (interaction['from'], interaction['to'])
            
            # 1. ì´ ê´€ê³„ê°€ ì²˜ìŒ ë°œê²¬ëœ ê²½ìš°, ì‚¬ì „ì— ì¶”ê°€í•©ë‹ˆë‹¤.
            if interaction_key not in prioritized_interactions:
                prioritized_interactions[interaction_key] = interaction
            else:
                # 2. ì´ë¯¸ ê´€ê³„ê°€ ì¡´ì¬í•  ê²½ìš°, ìš°ì„ ìˆœìœ„ ê·œì¹™ì„ ì ìš©í•©ë‹ˆë‹¤.
                existing_type = prioritized_interactions[interaction_key]['type']
                new_type = interaction['type']
                
                # ê¸°ì¡´ ê´€ê³„ê°€ 'agreement'ì´ê³  ìƒˆë¡œìš´ ê´€ê³„ê°€ 'disagreement'ì¼ ë•Œë§Œ êµì²´í•©ë‹ˆë‹¤.
                if existing_type == 'agreement' and new_type == 'disagreement':
                    prioritized_interactions[interaction_key] = interaction

        # ìµœì¢…ì ìœ¼ë¡œ í•„í„°ë§ëœ ìƒí˜¸ì‘ìš© ëª©ë¡ì„ ë”•ì…”ë„ˆë¦¬ì˜ ê°’ë“¤ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
        final_interactions = list(prioritized_interactions.values())
        
        logger.info(f"--- [Flow Analysis] Analysis complete. Found {len(interactions_list)} interactions, returning {len(final_interactions)} prioritized interactions. ---")
        return {"interactions": final_interactions} # ìš°ì„ ìˆœìœ„ê°€ ì ìš©ëœ ìµœì¢… ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜

    except Exception as e:
        logger.error(f"!!! [Flow Analysis] Error during interaction analysis: {e}", exc_info=True)
        return {"interactions": []}

# íˆ¬í‘œ ìƒì„±ì„ ìœ„í•œ ë³„ë„ì˜ í—¬í¼ í•¨ìˆ˜
async def _generate_vote_options(transcript_str: str, discussion_id: str, turn_number: int, vote_history: List[str], topic: str) -> Optional[dict]:
    """
    ëŒ€í™”ë¡ê³¼ í† ë¡  ì£¼ì œë¥¼ ë¶„ì„í•˜ì—¬ ìƒˆë¡œìš´ íˆ¬í‘œ ì£¼ì œì™€ ì„ íƒì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    - TypeError ë°©ì§€ë¥¼ ìœ„í•´ topic ì¸ìë¥¼ ë°›ìŠµë‹ˆë‹¤.
    - KeyError ë°©ì§€ë¥¼ ìœ„í•´ ëŒ€í™”ë¡ì˜ ì¤‘ê´„í˜¸ë¥¼ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    - JSON íŒŒì‹± ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ì•ˆì •ì ì¸ 2ë‹¨ê³„ íŒŒì‹± ë¡œì§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - í•œê¸€ ì˜¤íƒ€ ë°©ì§€ë¥¼ ìœ„í•´ 'í•µì‹¬ ìš©ì–´' ê°€ì´ë“œë¼ì¸ì„ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    """
    logger.info(f"--- [Vote Generation] Turn: {turn_number} íˆ¬í‘œ ìƒì„± ì‹œì‘ ---")
    raw_response = ""
    json_str = ""
    try:
        vote_caster_setting = await AgentSettings.find_one(
            AgentSettings.name == "Vote Caster", AgentSettings.status == "active"
        )
        if not vote_caster_setting:
            logger.error("!!! [Vote Generation] 'Vote Caster' ì—ì´ì „íŠ¸ë¥¼ DBì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # ì´ì „ íˆ¬í‘œ ê¸°ë¡ ì„¹ì…˜ ìƒì„±
        history_prompt_section = "ì•„ì§ ì‚¬ìš©ìì˜ ì´ì „ íˆ¬í‘œ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
        if vote_history:
            history_items = "\n".join([f"- '{item}'" for item in vote_history])
            history_prompt_section = f"### ì´ì „ íˆ¬í‘œì—ì„œ ì‚¬ìš©ìê°€ ì„ íƒí•œ í•­ëª©ë“¤:\n{history_items}"

        # 2. í† ë¡  ì£¼ì œë¥¼ ë°”íƒ•ìœ¼ë¡œ 'í•µì‹¬ ìš©ì–´' ê°€ì´ë“œë¼ì¸ì„ ë™ì ìœ¼ë¡œ ìƒì„± (í•µì‹¬ ìˆ˜ì •!)
        key_terms_guide = f"### Key Terms (Use these exact Korean spellings):\n- '{topic}'"

        # ëŒ€í™”ë¡ì— í¬í•¨ë  ìˆ˜ ìˆëŠ” ì¤‘ê´„í˜¸ë¥¼ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬í•˜ì—¬ KeyErrorë¥¼ ì›ì²œ ë°©ì§€
        safe_transcript_str = transcript_str.replace('{', '{{').replace('}', '}}')

        # ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°ë¦½
        final_human_prompt = (
            f"{key_terms_guide}\n\n" # <--- í•œ ì¤„ ë„ì›Œì„œ ê°€ë…ì„± í™•ë³´
            f"{history_prompt_section}\n\n"
            f"### í˜„ì¬ ë¼ìš´ë“œê¹Œì§€ì˜ ì „ì²´ í† ë¡  ëŒ€í™”ë¡:\n{safe_transcript_str}"
        )

        # LLM í˜¸ì¶œ
        vote_caster_agent = ChatGoogleGenerativeAI(
            model=vote_caster_setting.config.model,
            temperature=vote_caster_setting.config.temperature,
            model_kwargs={"response_mime_type": "application/json"}
        )
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", vote_caster_setting.config.prompt),
            ("human", final_human_prompt)
        ])
        
        chain = prompt | vote_caster_agent
        response = await chain.ainvoke(
            {},
            config={"tags": [f"discussion_id:{discussion_id}", f"turn:{turn_number}", "task:generate_vote"]}
        )

        raw_response = response.content
        
        # ì•ˆì •ì ì¸ 2ë‹¨ê³„ íŒŒì‹± ë¡œì§
        # 1. LLM ì‘ë‹µì—ì„œ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        match = re.search(r"```(json)?\s*({.*?})\s*```", raw_response, re.DOTALL)
        json_str = match.group(2) if match else raw_response

        # 2. Python ê¸°ë³¸ json ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë¨¼ì € íŒŒì‹±
        parsed_json = json.loads(json_str)
        
        # 3. íŒŒì‹±ëœ ë”•ì…”ë„ˆë¦¬ë¥¼ Pydanticìœ¼ë¡œ ê²€ì¦
        vote_content = VoteContent.model_validate(parsed_json)
        
        logger.info(f"--- [Vote Generation] íˆ¬í‘œ ìƒì„± ë° íŒŒì‹± ì™„ë£Œ: {vote_content.topic} ---")
        return vote_content.model_dump()
        
    except (ValidationError, json.JSONDecodeError) as e:
        logger.error(f"!!! [Vote Generation] JSON íŒŒì‹±/ê²€ì¦ ì˜¤ë¥˜. ì˜¤ë¥˜: {e}\nì›ë³¸ ì‘ë‹µ: {raw_response}", exc_info=True)
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ, ì‚¬ìš©ìì—ê²Œ ë‹¤ìŒ ë¼ìš´ë“œë¡œ ë„˜ì–´ê°ˆ ìˆ˜ ìˆëŠ” ê¸°ë³¸ ì˜µì…˜ ì œê³µ
        return {
            "topic": "ë‹¤ìŒìœ¼ë¡œ ì–´ë–¤ í† ë¡ ì„ ì§„í–‰í• ê¹Œìš”?",
            "options": ["ê°€ì¥ ì˜ê²¬ì´ ì—‡ê°ˆë¦¬ëŠ” ìŸì ì— ëŒ€í•´ ì¶”ê°€ ë°˜ë¡ ", "ìƒˆë¡œìš´ ê´€ì ì˜ ì „ë¬¸ê°€ ì¶”ê°€ íˆ¬ì… ìš”ì²­", "í˜„ì¬ê¹Œì§€ ë‚´ìš© ì¤‘ê°„ ìš”ì•½"]
        }
    except Exception as e:
        logger.error(f"!!! [Vote Generation] íˆ¬í‘œ ìƒì„± ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return None
    
async def execute_turn(discussion_log: DiscussionLog, user_vote: Optional[str] = None, model_overrides: Optional[Dict[str, str]] = None):
    """
    ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë‹¨ì¼ í† ë¡  í„´ì„ ì‹¤í–‰í•˜ê³ , ê²°ê³¼ë¥¼ DBì— ê¸°ë¡í•©ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ íˆ¬í‘œ ê¸°ë¡ì€ Redisë¥¼ í†µí•´ ì„¸ì…˜ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    logger.info(f"--- [BG Task] Executing turn for Discussion ID: {discussion_log.discussion_id} ---")

     # --- ì‚¬ìš©ì ì„ íƒ ëª¨ë¸ ì ìš© ë¡œì§ ---
    if model_overrides:
        logger.info(f"--- [BG Task] Applying model overrides: {model_overrides} ---")
        # discussion_logì— ì €ì¥ëœ ì°¸ì—¬ì ëª©ë¡ì„ ìˆœíšŒ
        for participant in discussion_log.participants:
            agent_name = participant.get('name')
            # ì‚¬ìš©ìê°€ ì„ íƒí•œ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
            if agent_name in model_overrides:
                new_model = model_overrides[agent_name]
                original_model = participant.get('model')
                # í•´ë‹¹ ì—ì´ì „íŠ¸ì˜ ëª¨ë¸ì„ ì‚¬ìš©ìê°€ ì„ íƒí•œ ëª¨ë¸ë¡œ êµì²´
                participant['model'] = new_model
                logger.info(f"--- [BG Task] Model for '{agent_name}' overridden: from '{original_model}' to '{new_model}' ---")

    # --- ì‚¬íšŒì ì•ˆë‚´ ë©”ì‹œì§€ ì¶”ê°€ ---
    # ì‚¬ìš©ì íˆ¬í‘œê°€ ìˆê³ , ì²« í„´(ëª¨ë‘ ë³€ë¡ )ì´ ì•„ë‹ ë•Œ ì‚¬íšŒì ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ ë¨¼ì € ì¶”ê°€í•©ë‹ˆë‹¤.
    if user_vote and discussion_log.turn_number > 0:
        round_name = "ëª¨ë‘ ë³€ë¡ " if discussion_log.turn_number == 1 else f"{discussion_log.turn_number - 1}ì°¨ í† ë¡ "
        
        # 'ì„(ë¥¼)' ì¡°ì‚¬ ì²˜ë¦¬
        last_char = user_vote[-1]
        postposition = "ì„" if (ord(last_char) - 0xAC00) % 28 > 0 else "ë¥¼"

        moderator_message = (
            f"{round_name}ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. "
            f"ì‚¬ìš©ìëŠ” '{discussion_log.current_vote['topic']}' íˆ¬í‘œì— "
            f"'{user_vote}'{postposition} ì„ íƒí•˜ì˜€ìŠµë‹ˆë‹¤. ë‹¤ìŒ í† ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤."
        )
        moderator_turn_data = {
            "agent_name": "ì‚¬íšŒì", 
            "message": moderator_message, 
            "timestamp": datetime.utcnow()
        }
        discussion_log.transcript.append(moderator_turn_data)
    
    redis_key = f"vote_history:{discussion_log.discussion_id}"
    vote_history = []
    try:
        history_json = await db.redis_client.get(redis_key)
        if history_json:
            vote_history = json.loads(history_json)
    except Exception as e:
        logger.error(f"!!! [Redis Error] Redisì—ì„œ íˆ¬í‘œ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

    if user_vote:
        vote_history.append(user_vote)
        try:
            await db.redis_client.set(redis_key, json.dumps(vote_history), ex=86400)
            logger.info(f"--- [BG Task] ì‚¬ìš©ì íˆ¬í‘œ '{user_vote}'ë¥¼ Redisì— ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ ê¸°ë¡: {vote_history} ---")
        except Exception as e:
            logger.error(f"!!! [Redis Error] Redisì— íˆ¬í‘œ ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

    # --- ì¤‘ì•™ ê²€ìƒ‰ ë¡œì§ ì‹œì‘ ---
    central_search_results_str = ""
    # ì²« í„´(ëª¨ë‘ ë³€ë¡ )ì´ ì•„ë‹ˆë©´ì„œ ì‚¬ìš©ì íˆ¬í‘œê°€ ìˆì„ ë•Œë§Œ ê²€ìƒ‰ ìˆ˜í–‰
    if discussion_log.turn_number > 0 and user_vote:
        search_query = await _get_search_query(discussion_log, user_vote)
        if search_query:
            # search.pyì˜ ë¹„ë™ê¸° í•¨ìˆ˜ ì‚¬ìš©
            search_results = await perform_web_search_async(search_query)
            if search_results:
                formatted_results = "\n".join([
                    f"- ì¶œì²˜: {res['url']}\n- ìš”ì•½: {res['content']}" 
                    for res in search_results
                ])
                central_search_results_str = f"--- [ì¤‘ì•™ ì§‘ì¤‘ì‹ ì›¹ ê²€ìƒ‰ ê²°ê³¼]\n{formatted_results}\n---"

    current_turn = discussion_log.turn_number
    history_str = "\n\n".join([f"{t['agent_name']}: {t['message']}" for t in discussion_log.transcript])

    # DBì— ì €ì¥ëœ ì¦ê±° ìë£Œë¥¼ ë¶ˆëŸ¬ì™€ í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•  ë¬¸ìì—´ë¡œ ë§Œë“­ë‹ˆë‹¤.
    evidence_str = ""
    if discussion_log.evidence_briefing:
        web_evidence = "\n".join([f"- {item['summary']} (ì¶œì²˜: {item['source']})" for item in discussion_log.evidence_briefing.get('web_evidence', [])])
        file_evidence = "\n".join([f"- {item['summary']} (ì¶œì²˜: {item['source']})" for item in discussion_log.evidence_briefing.get('file_evidence', [])])
        
        evidence_str += "--- [ì°¸ê³  ìë£Œ: ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½] ---\n"
        evidence_str += web_evidence + "\n" if web_evidence else "ê´€ë ¨ ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
        evidence_str += "--- [ì°¸ê³  ìë£Œ: ì œì¶œ íŒŒì¼ ìš”ì•½] ---\n"
        evidence_str += file_evidence + "\n" if file_evidence else "ì œì¶œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\n"

    # ì¤‘ì•™ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ì¡´ evidence_strì— ì¶”ê°€í•©ë‹ˆë‹¤.
    if central_search_results_str:
        evidence_str += "\n\n" + central_search_results_str

    special_directive = ""
    if user_vote:
        special_directive = (
            f"\n\n--- íŠ¹ë³„ ì§€ì‹œë¬¸ ---\n"
            f"ì‚¬ìš©ìëŠ” ì§ì „ ë¼ìš´ë“œì—ì„œ '{user_vote}' ê´€ì ì— ëŒ€í•œ ë‹¹ì‹ ì˜ ì˜ê²¬ì„ ë“£ê³ ì‹¶ì–´í•©ë‹ˆë‹¤."
            f"ì´ ê´€ì ì„ í¬í•¨í•˜ì—¬ ë‹¹ì‹ ì˜ ì£¼ì¥ì„ ê°•í™” ë˜ëŠ” ìˆ˜ì •í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë“¤ì˜ ì£¼ì¥ì„ ë°˜ë°•í•˜ì‹­ì‹œì˜¤."
            f"ê·¸ëŸ¬ë‚˜ ì´ ê´€ì ì´ ë‹¹ì‹ ì˜ ìƒê°ì— ì¤‘ìš”í•˜ì§€ ì•Šë‹¤ë©´ ì´ í† ë¡ ì´ ì–´ë–¤ ë°©í–¥ì— ì¤‘ì ì„ ë‘ì–´ì•¼ í•˜ëŠ”ì§€ ì•ìœ¼ë¡œ ì–´ë–¤ ë…¼ì˜ë¥¼ ì‹¬í™” ë°œì „ì‹œì¼œì•¼ í•˜ëŠ”ì§€ ì˜¤íˆë ¤ ì ê·¹ì ìœ¼ë¡œ ë‹¹ì‹ ì˜ ì£¼ì¥ì„ í¼ì¹ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
            f"\n-------------------\n"
        )

    excluded_roles = ["ì¬íŒê´€", "ì‚¬íšŒì"]
    jury_members = [p for p in discussion_log.participants if p.get('name') not in excluded_roles]

    # --- ì—ì´ì „íŠ¸ ë°œì–¸ì„ ìˆœì°¨ ì‹¤í–‰ì—ì„œ ë™ì‹œ ì‹¤í–‰ìœ¼ë¡œ ë³€ê²½ ---

    # 1. ëª¨ë“  ì—ì´ì „íŠ¸ì˜ ë¹„ë™ê¸° ì‘ì—…ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    tasks = []
    for agent_config in jury_members:
        # 2. awaitë¡œ ì¦‰ì‹œ ì‹¤í–‰í•˜ëŠ” ëŒ€ì‹ , ì‹¤í–‰í•  ì‘ì—…(ì½”ë£¨í‹´)ì„ ë§Œë“¤ì–´ tasks ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        task = _run_single_agent_turn(
            agent_config, 
            discussion_log.topic, 
            history_str, 
            evidence_str,  # ì¦ê±° ìë£Œ ì „ë‹¬
            special_directive,
            discussion_log.discussion_id,
            current_turn
        )
        tasks.append(task)

    # 3. asyncio.gatherë¥¼ ì‚¬ìš©í•´ ëª¨ë“  ì‘ì—…ì„ ë™ì‹œì— ì‹¤í–‰í•˜ê³ , ëª¨ë“  ê²°ê³¼ê°€ ë„ì°©í•  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
    logger.info(f"--- [BG Task] {len(tasks)}ëª…ì˜ ì—ì´ì „íŠ¸ ë°œì–¸ì„ ë™ì‹œì— ìƒì„± ì‹œì‘... (ID: {discussion_log.discussion_id})")
    messages = await asyncio.gather(*tasks)
    logger.info(f"--- [BG Task] ëª¨ë“  ì—ì´ì „íŠ¸ ë°œì–¸ ìƒì„± ì™„ë£Œ. (ID: {discussion_log.discussion_id})")

    # 4. ì´ì œ ëª¨ë“  ë‹µë³€ì´ ë„ì°©í–ˆìœ¼ë¯€ë¡œ, ê²°ê³¼ë¥¼ ìˆœì„œëŒ€ë¡œ transcriptì— ì¶”ê°€í•©ë‹ˆë‹¤.
    for i, message in enumerate(messages):
        agent_name = jury_members[i]['name']
        
        # 1. ì „ë¬¸ê°€ì˜ ë©”ì¸ ë°œì–¸ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        main_turn_data = {"agent_name": agent_name, "message": message, "timestamp": datetime.utcnow()}
        discussion_log.transcript.append(main_turn_data)

        # 2. ë©”ì¸ ë°œì–¸ì— ëŒ€í•´ Staff ì—ì´ì „íŠ¸ë“¤ì„ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
        # (asyncio.to_threadë¥¼ ì‚¬ìš©í•´ non-blocking ë°©ì‹ìœ¼ë¡œ í˜¸ì¶œ)
        snr_result = await asyncio.to_thread(run_snr_agent, message)
        if snr_result:
            snr_turn_data = {
                "agent_name": "SNR ì „ë¬¸ê°€", 
                "message": json.dumps(snr_result, ensure_ascii=False), # ê²°ê³¼ë¥¼ JSON ë¬¸ìì—´ë¡œ ì €ì¥
                "timestamp": datetime.utcnow()
            }
            discussion_log.transcript.append(snr_turn_data)

        verifier_result = await asyncio.to_thread(run_verifier_agent, message)
        if verifier_result:
            verifier_turn_data = {
                "agent_name": "ì •ë³´ ê²€ì¦ë¶€", 
                "message": json.dumps(verifier_result, ensure_ascii=False), # ê²°ê³¼ë¥¼ JSON ë¬¸ìì—´ë¡œ ì €ì¥
                "timestamp": datetime.utcnow()
            }
            discussion_log.transcript.append(verifier_turn_data)

    # --- ë¼ìš´ë“œ ì¢…ë£Œ êµ¬ë¶„ì„  ì¶”ê°€] ---
    round_name_for_separator = "ëª¨ë‘ ë³€ë¡ " if discussion_log.turn_number == 0 else f"{discussion_log.turn_number}ì°¨ í† ë¡ "
    separator_message = f"---------- {round_name_for_separator} ì¢…ë£Œ ----------"
    separator_turn_data = {
        "agent_name": "êµ¬ë¶„ì„ ", 
        "message": separator_message, 
        "timestamp": datetime.utcnow()
    }
    discussion_log.transcript.append(separator_turn_data)
    
    logger.info(f"--- [BG Task] ë¼ìš´ë“œ {current_turn} ì™„ë£Œ. ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ID: {discussion_log.discussion_id})")
    
    # ë¶„ì„ì— í•„ìš”í•œ ìµœì‹  ëŒ€í™”ë¡ ë¬¸ìì—´ ìƒì„± (ì´ë²ˆ ë¼ìš´ë“œ ë°œì–¸ë§Œ)
    final_transcript_str = "\n\n".join([f"{t['agent_name']}: {t['message']}" for t in discussion_log.transcript[-len(jury_members):]])
    
    analysis_tasks = {
        "round_summary": _get_round_summary(final_transcript_str, discussion_log.discussion_id, discussion_log.turn_number),
        "stance_changes": _analyze_stance_changes(discussion_log.transcript, jury_members, discussion_log.discussion_id, discussion_log.turn_number),
        "flow_data": _analyze_flow_data(discussion_log.transcript, jury_members, discussion_log.discussion_id, discussion_log.turn_number)
    }
    
    analysis_results = await asyncio.gather(*analysis_tasks.values())
    analysis_map = dict(zip(analysis_tasks.keys(), analysis_results))

    discussion_log.round_summary = {
        "critical_utterance": analysis_map.get("round_summary"),
        "stance_changes": analysis_map.get("stance_changes")
    }
    discussion_log.flow_data = analysis_map.get("flow_data")

    logger.info(f"--- [BG Task] ë¶„ì„ ì™„ë£Œ. ê²°ê³¼ë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤. (ID: {discussion_log.discussion_id})")
    
    # ë‹¤ìŒ ë¼ìš´ë“œë¥¼ ìœ„í•œ íˆ¬í‘œ ìƒì„±
    full_history_str = "\n\n".join([f"{t['agent_name']}: {t['message']}" for t in discussion_log.transcript])
    discussion_log.current_vote = await _generate_vote_options(
        full_history_str, 
        discussion_log.discussion_id, 
        discussion_log.turn_number,
        vote_history,
        discussion_log.topic
    )
    
    discussion_log.status = "waiting_for_vote"
    discussion_log.turn_number += 1
    
    await discussion_log.save()
    
    logger.info(f"--- [BG Task] Turn completed for {discussion_log.discussion_id}. New status: '{discussion_log.status}' ---")

# ëª¨ë¸ ì´ë¦„ì— ë”°ë¼ ì ì ˆí•œ LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜ ì¶”ê°€
def get_llm_client(model_name: str, temperature: float):
    """ëª¨ë¸ ì´ë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜¬ë°”ë¥¸ LangChain LLM í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if model_name.startswith("gemini"):
        return ChatVertexAI(
            model_name=model_name, 
            temperature=temperature,
            location="asia-northeast3"
        )
    elif model_name.startswith("gpt"):
        return ChatOpenAI(model=model_name, temperature=temperature)
    elif model_name.startswith("claude"):
        return ChatAnthropic(model=model_name, temperature=temperature)
    else:
        # ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ ì´ë¦„ì¼ ê²½ìš°, ê¸°ë³¸ ëª¨ë¸ë¡œ ëŒ€ì²´í•˜ì—¬ ì˜¤ë¥˜ ë°©ì§€
        logger.warning(f"Unrecognized model name '{model_name}'. Falling back to Gemini 1.5 pro.")
        return ChatGoogleGenerativeAI(model="gemini-1.5-pro", temperature=temperature)